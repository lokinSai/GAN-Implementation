{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook is creadited to Jason Brownlee and was created by walking through his article [How to Train a Progressive Growing GAN in Keras for Synthesizing Faces](https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the CelebA and cartoon datasets to synthesize images using a progressive GAN architecture. We found an informative article with a walkthrough of code and architecture setup to train progressive GANs.This article provided a lot of great explantion of the complex architecture and how to implement the custom layers that allow for the progressive learning. We used this architecture as a comparison to our model and the Info-GAN architecture.  \n",
    "  \n",
    "One of the main improvements that Progressive GANs provide is the incremental increase in the size of the images output by the generator. The training procedure will fine-tune a given output size and then slowly fade in a new model with a slightly larger resolution size. During this process all layers remain trainable which adds to the efficiency and perforamane of progressive GANs  \n",
    "  \n",
    "##### Note: throughout this notebook you will need to point the locations of the data and models to load/save to your local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the Progressive GAN on the CelebA dataset and a cartoon dataset. You can download the CelebA dataset [here](https://www.kaggle.com/jessicali9530/celeba-dataset)\n",
    "\n",
    "Progressive GANs work best with small square images and since we are only concerned with generating faces, we can remove any background pixels using the [MTCNN](https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/) face detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend, constraints, initializers, layers, models as KM, optimizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numpy import asarray, load, ones, savez_compressed, random, zeros\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multi-Task Cascaded Convolutional Neural Network (MTCNN) to detect faces in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    \"\"\"Loads an image as an RGB numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        The name of the file from the source dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of pixel values from the input file image\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    pixels = asarray(image)\n",
    "\n",
    "    return pixels\n",
    "\n",
    "\n",
    "def extract_face(model, pixels, required_size=(128, 128)):\n",
    "    \"\"\"Extracts face from loaded image with MTCNN and resizes to square\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: mtcnn.MTCNN\n",
    "        The MTCNN model used to detect faces in input image\n",
    "        \n",
    "    pixels: numpy.ndarray\n",
    "        Array of pixel values from input image\n",
    "    \n",
    "    required_size: tuple\n",
    "        The resized image size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of the resized face detected from the image\n",
    "    \"\"\"\n",
    "\n",
    "    faces = model.detect_faces(pixels)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the first face if more than one are detected in the image\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "\n",
    "    face_pixels = pixels[y1:y2, x1:x2]\n",
    "\n",
    "    image = Image.fromarray(face_pixels).resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "\n",
    "    return face_array\n",
    " \n",
    "\n",
    "def load_face_data(dataset_dir, num_faces):\n",
    "    \"\"\"Load in dataset and extract faces from all images\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_dir: str\n",
    "        The directory that cotains the image dataset\n",
    "        \n",
    "    num_faces: int\n",
    "        The number of faces wanted to train\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of the faces for training\n",
    "    \"\"\"\n",
    "\n",
    "    model = MTCNN()\n",
    "    faces = list()\n",
    "\n",
    "    for filename in listdir(dataset_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        pixels = load_image(dataset_dir + filename)\n",
    "        face = extract_face(model, pixels)\n",
    "\n",
    "        if face is None:\n",
    "            continue\n",
    "\n",
    "        faces.append(face)\n",
    "\n",
    "        if len(faces) >= num_faces:\n",
    "            break\n",
    "\n",
    "    return asarray(faces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load dataset\n",
    "2. Detect faces\n",
    "3. Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = 'img_align_celeba/'\n",
    "# dataset_directory = 'cartoonset10k/'\n",
    "num_faces_for_training = 50000\n",
    "\n",
    "all_faces = load_face_data(dataset_directory, num_faces_for_training)\n",
    "print('Loaded: ', all_faces.shape)\n",
    "\n",
    "savez_compressed('img_align_celeba_128.npz', all_faces)\n",
    "# savez_compressed('cartoon.npz', all_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can now be loaded in for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "data = load('img_align_celeba_128_5000.npz')\n",
    "# data = load('cartoon.npz')\n",
    "\n",
    "faces = data['arr_0']\n",
    "print('Loaded: ', faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pyplot.rcParams['figure.figsize'] = (15, 15)\n",
    "\n",
    "def plot_faces(faces, n):\n",
    "    for i in range(n * n):\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(faces[i].astype('uint8'))\n",
    "    pyplot.show()\n",
    "\n",
    "print('Loaded: ', faces.shape)\n",
    "plot_faces(faces, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "In this approach, each phase of growth is defined as its own model. For this to work properly, there are several custom layers that need to be implemented:  \n",
    "1. PixelNormalization - used to normalize activations in generator  \n",
    "2. MinibatchStdev - Summerizes batch statistics from images in discriminator  \n",
    "3. WeightedSum - Controls the weighted sum of old and new layers during the phase-in process of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Normalization Layer:  \n",
    "\n",
    "\n",
    "In progressive GANs, instead of using a batch normalization like many other GAN architectures, each pixel in the activations is normalized to unit length. [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196) by Tero Karras, et al. defines this as pixelwise feature vector normalization. It's good to note that in this architecture we are only normalizing the generator model and not the discriminator.\n",
    "\n",
    "This layer is used between each convolutional layer and activation function in the generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom Keras layer used to normalize the activation map in the generator\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Layer : keras.layers.Layer\n",
    "        Parent class used for implementing a custom Keras layer\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        Compute the normalized activations\n",
    "        \n",
    "    compute_output_shape(input_shape)\n",
    "        Allows keras to do automatic shape inference. In this layer \n",
    "        the input shape is the same as the output shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Computes the normalized activations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            array of the input activations\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            An array of the normalized activations\n",
    "        \"\"\"\n",
    "\n",
    "        values = inputs**2.0\n",
    "        mean_values = backend.mean(values, axis=-1, keepdims=True)\n",
    "        mean_values += 1.0e-8\n",
    "        l2 = backend.sqrt(mean_values)\n",
    "        normalized = inputs / l2\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Define the shape of the output\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            array of the input shape\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            An array of output layer shape\n",
    "        \"\"\"\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Standard Deviation Layer:  \n",
    "\n",
    "This custom layer summerizes a batch of activations in the output layer of discriminator. With this approach, the discriminator learns to better detect batches of fake samples from batches of real samples. The encoourages the generator to create batches of samples with realistic batch satatistics. \n",
    "\n",
    "  \n",
    "The activation batch statistics are gathered by calculating the standard deviation of each pixel value in the activation and then calculating the avearage of that value. This results in a single channel activation map that is appened to the input list of activation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStdev(layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom Keras layer used to normalize the activation map in the generator\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Layer : keras.layers.Layer\n",
    "        Parent class used for implementing a custom Keras layer\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        Compute the batch activation statistics\n",
    "        \n",
    "    compute_output_shape(input_shape)\n",
    "        Allows keras to do automatic shape inference. In this layer \n",
    "        the input shape is the same as the output shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Compute the batch activation statistics\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            array of the input activations\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            An activation map with the statistic summary appended\n",
    "        \"\"\"\n",
    "        mean = backend.mean(inputs, axis=0, keepdims=True)\n",
    "        squ_diffs = backend.square(inputs - mean)\n",
    "        mean_sq_diff = backend.mean(squ_diffs, axis=0, keepdims=True)\n",
    "        mean_sq_diff += 1e-8\n",
    "        stdev = backend.sqrt(mean_sq_diff)\n",
    "        mean_pix = backend.mean(stdev, keepdims=True)\n",
    "        shape = backend.shape(inputs)\n",
    "        output = backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
    "\n",
    "        combined = backend.concatenate([inputs, output], axis=-1)\n",
    "\n",
    "        return combined\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Added channel to output shape for batch statistics\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            tuple of the input shape\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            tuple of output layer shape\n",
    "        \"\"\"\n",
    "        input_shape = list(input_shape)\n",
    "        input_shape[-1] += 1\n",
    "\n",
    "        return tuple(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Sum Layer:  \n",
    "\n",
    "This custom layer merges the activations from two input layers using a variable called alpha that controls how much to weight the first and second inputs. It is used in both the generator and discriminator during the growth phase of training when the model is transitioning from one image size to a new image size (e.g., from 4×4 to 8×8 pixels).\n",
    "\n",
    "The alpha parameter is linearly scaled from 0.0 at the beginning to 1.0 at the end, allowing the output of the layer to transition from giving full weight to the old layers to giving full weight to the new layers (second input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(layers.Add):\n",
    "    \"\"\"\n",
    "    A custom Keras layer extending the Add merge layer used to \n",
    "    merge the activations of two input layers\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Add : keras.layers.Add\n",
    "        Parent class used for implementing a custom Keras Add merge layer\n",
    "    \n",
    "    alpha: used to linearly scale the weight on each of the two inputs\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    merge_fun(inputs)\n",
    "        Compute the batch activation statistics\n",
    "        \n",
    "    compute_output_shape(input_shape)\n",
    "        Allows keras to do automatic shape inference. In this layer \n",
    "        the input shape is the same as the output shape\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        # merges two input layers based on a linear alpha value\n",
    "        assert (len(inputs) == 2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_hat):\n",
    "    \"\"\"Calculates wasserstein loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Tensor\n",
    "        The true labels\n",
    "        \n",
    "    y_hat: Tensor\n",
    "        The predicted values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        A tensor of loss values\n",
    "    \"\"\"\n",
    "    return backend.mean(y_true * y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_discriminator_block(src_model, n_input_layers=3):\n",
    "    \"\"\"Takes a trained discriminator model and creates a new growth model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src_model: keras.models.Model\n",
    "        The old model prior to growth\n",
    "        \n",
    "    n_input_layers: int\n",
    "        The number of input layers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of two models - one growth model with two pathways and the WeightedSum layer, \n",
    "        and a copy of the src_model without the 1x1 layer and the WeightedSum layer\n",
    "    \"\"\"\n",
    "\n",
    "    init = initializers.RandomNormal(stddev=0.02)\n",
    "    const = constraints.max_norm(1.0)\n",
    "\n",
    "    in_shape = list(src_model.input.shape)\n",
    "    # print('in_shape: {}'.format(in_shape[-2].value))\n",
    "\n",
    "    input_shape = (in_shape[-2]*2, in_shape[-2]*2, in_shape[-1])\n",
    "    in_image = layers.Input(shape=input_shape)\n",
    "\n",
    "    d = layers.Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = layers.AveragePooling2D()(d)\n",
    "    block_new = d\n",
    "\n",
    "    for i in range(n_input_layers, len(src_model.layers)):\n",
    "        d = src_model.layers[i](d)\n",
    "\n",
    "    model1 = KM.Model(in_image, d)\n",
    "\n",
    "    model1.compile(loss=wasserstein_loss,\n",
    "                   optimizer=optimizers.Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\n",
    "    downsample = layers.AveragePooling2D()(in_image)\n",
    "\n",
    "    block_old = src_model.layers[1](downsample)\n",
    "    block_old = src_model.layers[2](block_old)\n",
    "\n",
    "    d = WeightedSum()([block_old, block_new])\n",
    "\n",
    "    for i in range(n_input_layers, len(src_model.layers)):\n",
    "        d = src_model.layers[i](d)\n",
    "\n",
    "    model2 = KM.Model(in_image, d)\n",
    "\n",
    "    model2.compile(loss=wasserstein_loss,\n",
    "                   optimizer=optimizers.Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\n",
    "    return [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(n_phases, input_shape=(4,4,3)):\n",
    "    \"\"\"Defines base discriminator model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_phases: int\n",
    "        The number of phases the model with have (e.g., 4 - 4x4, 8x8, 16x16, 32x32)\n",
    "        \n",
    "    input_shape: tuple\n",
    "        The tuple shape of the input\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of models for each of the N phases\n",
    "    \"\"\"\n",
    "\n",
    "    init = initializers.RandomNormal(stddev=0.02)\n",
    "    const = constraints.max_norm(1.0)\n",
    "\n",
    "    model_list = list()\n",
    "    in_image = layers.Input(shape=input_shape)\n",
    "\n",
    "    d = layers.Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = MinibatchStdev()(d)\n",
    "    d = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = layers.Conv2D(128, (4,4), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = layers.Flatten()(d)\n",
    "    out_class = layers.Dense(1)(d)\n",
    "\n",
    "    model = KM.Model(in_image, out_class)\n",
    "\n",
    "    model.compile(loss=wasserstein_loss,\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\n",
    "    model_list.append([model, model])\n",
    "\n",
    "    for i in range(1, n_phases):\n",
    "        src_model = model_list[i - 1][0]\n",
    "        models = add_discriminator_block(src_model)\n",
    "        model_list.append(models)\n",
    "\n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_generator_block(src_model):\n",
    "    \"\"\"Takes a trained geneartor model and creates a new growth model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src_model: keras.models.Model\n",
    "        The old model prior to growth\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of two models - one growth model with two pathways and the WeightedSum layer, \n",
    "        and a copy of the src_model without the 1x1 layer and the WeightedSum layer\n",
    "    \"\"\"\n",
    "\n",
    "    init = initializers.RandomNormal(stddev=0.02)\n",
    "    const = constraints.max_norm(1.0)\n",
    "    block_end = src_model.layers[-2].output\n",
    "    upsampling = layers.UpSampling2D()(block_end)\n",
    "\n",
    "    g = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = layers.LeakyReLU(alpha=0.2)(g)\n",
    "    g = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    out_image = layers.Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\n",
    "    model1 = KM.Model(src_model.input, out_image)\n",
    "\n",
    "    out_old = src_model.layers[-1]\n",
    "    out_image2 = out_old(upsampling)\n",
    "\n",
    "    merged = WeightedSum()([out_image2, out_image])\n",
    "    model2 = models.Model(src_model.input, merged)\n",
    "\n",
    "    return [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, n_phases, in_dim=4):\n",
    "    \"\"\"Defines base generator model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_dim: int\n",
    "        The number of latent datapoints as input\n",
    "\n",
    "    n_phases: int\n",
    "        The number of phases the model with have (e.g., 4 - 4x4, 8x8, 16x16, 32x32)\n",
    "\n",
    "    in_dim: int\n",
    "        The input dimension\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of models for each of the N phases\n",
    "    \"\"\"\n",
    "\n",
    "    init = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "    const = constraints.max_norm(1.0)\n",
    "    model_list = list()\n",
    "\n",
    "    in_latent = layers.Input(shape=(latent_dim,))\n",
    "\n",
    "    g = layers.Dense(128 * in_dim * in_dim, kernel_initializer=init, kernel_constraint=const)(in_latent)\n",
    "    g = layers.Reshape((in_dim, in_dim, 128))(g)\n",
    "\n",
    "    g = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    g = layers.Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "    g = PixelNormalization()(g)\n",
    "    g = layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    out_image = layers.Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\n",
    "    model = KM.Model(in_latent, out_image)\n",
    "    model_list.append([model, model])\n",
    "\n",
    "    for i in range(1, n_phases):\n",
    "        src_model = model_list[i - 1][0]\n",
    "        models = add_generator_block(src_model)\n",
    "        model_list.append(models)\n",
    "\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_composite(discriminators, generators):\n",
    "    \"\"\"Defines the full composit model including the discrimnator and generator phases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    discriminators: list\n",
    "        list of discriminator phase models\n",
    "\n",
    "    generators: list\n",
    "        list of generator phase models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of composit models for each of the N phases\n",
    "    \"\"\"\n",
    "    model_list = list()\n",
    "\n",
    "    for i in range(len(discriminators)):\n",
    "        g_models, d_models = generators[i], discriminators[i]\n",
    "        d_models[0].trainable = False\n",
    "        model1 = models.Sequential()\n",
    "        model1.add(g_models[0])\n",
    "        model1.add(d_models[0])\n",
    "        model1.compile(loss=wasserstein_loss,\n",
    "                       optimizer=optimizers.Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\n",
    "        d_models[1].trainable = False\n",
    "        model2 = models.Sequential()\n",
    "        model2.add(g_models[1])\n",
    "        model2.add(d_models[1])\n",
    "        model2.compile(loss=wasserstein_loss,\n",
    "                       optimizer=optimizers.Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "        model_list.append([model1, model2])\n",
    "\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_real_samples(filename):\n",
    "    \"\"\"Loads real samples from training dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        The filename of the sample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A scaled version of the sample from [0, 255] to [-1, 1]\n",
    "    \"\"\"\n",
    "    data = load(filename)\n",
    "    X = data['arr_0'].astype('float32')\n",
    "    X = (X - 127.5) / 127.5\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    \"\"\"Selects random real samples form dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: numpy.ndarray\n",
    "        The source dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A tuple of features and labels\n",
    "    \"\"\"\n",
    "    ix = random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    \"\"\"Generate random points in latent space as input for the generator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_dim: int\n",
    "        number of latent points per sample\n",
    "\n",
    "    n_samples: int\n",
    "        number of samples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        A batch of inputs for network\n",
    "    \"\"\"\n",
    "\n",
    "    x_input = random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "\n",
    "    return x_input\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    \"\"\"Generate fake samples with generator with labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generator: keras.models.Model\n",
    "        generator model\n",
    "\n",
    "    latent_dim: int\n",
    "        number of latent points per sample\n",
    "\n",
    "    n_samples: int\n",
    "        number of samples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Fake image samples with class labels\n",
    "    \"\"\"\n",
    "\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = -ones((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def update_fadein(models, step, n_steps):\n",
    "    \"\"\"Update the alpha values on each instance of WeightedSum layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models: list\n",
    "        list of models to update the WeightedSum layer for\n",
    "\n",
    "    step: int\n",
    "        the intensity of the increase in alpha each turn\n",
    "\n",
    "    n_steps: int\n",
    "        number of steps to get from 0 - 1\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = step / float(n_steps - 1)\n",
    "    for model in models:\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, WeightedSum):\n",
    "                backend.set_value(layer.alpha, alpha)\n",
    "\n",
    "\n",
    "def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, fadein=False):\n",
    "    \"\"\"Train each model for a given epochs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    g_model: keras.models.Model\n",
    "        generator model\n",
    "\n",
    "    d_model: keras.models.Model\n",
    "        discriminator model\n",
    "\n",
    "    gan_model: keras.models.Model\n",
    "        composite model\n",
    "    \n",
    "    dataset: numpy.ndarray\n",
    "        source dataset\n",
    "    \n",
    "    n_epochs: int\n",
    "        number of epochs to train over\n",
    "        \n",
    "    n_batch: int\n",
    "        sample batch size\n",
    "    \n",
    "    fadein: boolean\n",
    "        flag denoting if this is a fadein phase of training\n",
    "    \"\"\"\n",
    "\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    \n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        if fadein:\n",
    "            update_fadein([g_model, d_model, gan_model], i, n_steps)\n",
    "\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "        z_input = generate_latent_points(latent_dim, n_batch)\n",
    "        y_real2 = ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    "        # print('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "\n",
    "def scale_dataset(images, new_shape):\n",
    "    \"\"\"scale images to a new size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images: list\n",
    "        source dataset images\n",
    "\n",
    "    new_shape: tuple\n",
    "        new images size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of resized images\n",
    "    \"\"\"\n",
    "\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)\n",
    "\n",
    "\n",
    "def summarize_performance(status, g_model, latent_dim, n_samples=25):\n",
    "    \"\"\"generate samples at each phase and save models and plots\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    status: str\n",
    "        training phase status\n",
    "\n",
    "    g_model: keras.models.Model\n",
    "        generator model\n",
    "    \n",
    "    latent_dim:\n",
    "        dimension of latent points\n",
    "    \n",
    "    n_samples:\n",
    "        number of samples to generate from model\n",
    "    \"\"\"\n",
    "\n",
    "    gen_shape = g_model.output_shape\n",
    "    name = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n",
    "\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    square = int(sqrt(n_samples))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(square, square, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X[i])\n",
    "\n",
    "    filename1 = 'celeba__5000_plot_%s.png' % (name)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "\n",
    "    filename2 = 'celeba_5000_model_%s.h5' % (name)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "    \n",
    "\n",
    "def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n",
    "    \"\"\"Train the discriminator, generator, and GAN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g_models: list\n",
    "        list of generator models\n",
    "\n",
    "    d_models: list\n",
    "        list of discriminator models\n",
    "    \n",
    "    gan_modles: list\n",
    "        list of composit models\n",
    "    \n",
    "    dataset: numpy.ndarray\n",
    "        source dataset\n",
    "        \n",
    "    latent_dim: int\n",
    "        number of latent points\n",
    "    \n",
    "    e_norm: int\n",
    "        training epoch for fine-tuned phase\n",
    "    \n",
    "    e_fadin: int\n",
    "        training epochs for fade-in phase\n",
    "\n",
    "    n_batch: int\n",
    "        batch size\n",
    "    \"\"\"\n",
    "\n",
    "    g_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
    "    gen_shape = g_normal.output_shape\n",
    "    scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "    print('Scaled Data', scaled_data.shape)\n",
    "    # train normal or straight-through models\n",
    "    train_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[0], n_batch[0])\n",
    "    summarize_performance('tuned', g_normal, latent_dim)\n",
    "\n",
    "    for i in range(1, len(g_models)):\n",
    "        [g_normal, g_fadein] = g_models[i]\n",
    "        [d_normal, d_fadein] = d_models[i]\n",
    "        [gan_normal, gan_fadein] = gan_models[i]\n",
    "        gen_shape = g_normal.output_shape\n",
    "        scaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "        print('Scaled Data', scaled_data.shape)\n",
    "\n",
    "        # train fade-in models for next level of growth\n",
    "        train_epochs(g_fadein, d_fadein, gan_fadein, scaled_data, e_fadein[i], n_batch[i], True)\n",
    "        summarize_performance('faded', g_fadein, latent_dim)\n",
    "\n",
    "        # train normal or straight-through models\n",
    "        train_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[i], n_batch[i])\n",
    "        summarize_performance('tuned', g_normal, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_growth_phases = 6 # [4, 8, 16, 32, 64, 128]\n",
    "latent_dims = 100\n",
    "\n",
    "d_models = define_discriminator(num_growth_phases)\n",
    "print('d_models defined...')\n",
    "\n",
    "g_models = define_generator(latent_dims, num_growth_phases)\n",
    "print('g_models defined...')\n",
    "\n",
    "gan_models = define_composite(d_models, g_models)\n",
    "print('gan_models defined...')\n",
    "\n",
    "dataset = load_real_samples('img_align_celeba_128.npz')\n",
    "# dataset = load_real_samples('cartoon.npz')\n",
    "\n",
    "print('Loaded', dataset.shape)\n",
    "\n",
    "n_batch = [16, 16, 16, 8, 4, 4]\n",
    "n_epochs = [5, 8, 8, 10, 10, 10]\n",
    "\n",
    "train(g_models, d_models, gan_models, dataset, latent_dims, n_epochs, n_epochs, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images from Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated(images, n_images):\n",
    "    \"\"\"Create a plot of generated images\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images: numpy.ndarray\n",
    "        array of images\n",
    "\n",
    "    n_images: int\n",
    "        number of images\n",
    "    \"\"\"\n",
    "    square = int(sqrt(n_images))\n",
    "    images = (images - images.min()) / (images.max() - images.min())\n",
    "\n",
    "    for i in range(n_images):\n",
    "        pyplot.subplot(square, square, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(images[i])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_layers = {'PixelNormalization': PixelNormalization, 'MinibatchStdev': MinibatchStdev, 'WeightedSum': WeightedSum}\n",
    "model = KM.load_model('model_064x064-tuned.h5', custom_layers)\n",
    "\n",
    "latent_dim = 100\n",
    "n_images = 25\n",
    "latent_points = generate_latent_points(latent_dim, n_images)\n",
    "X  = model.predict(latent_points)\n",
    "plot_generated(X, n_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
